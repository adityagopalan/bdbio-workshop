{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and moved to device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Load GPT-2 model and tokenizer directly\n",
    "model_id = \"gpt2\"  # This is the 124M parameter version\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float32,  # GPT-2 small works fine in full precision\n",
    ")\n",
    "\n",
    "# Add a proper pad token for GPT-2\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(\"Model loaded and moved to device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 124,440,576\n"
     ]
    }
   ],
   "source": [
    "# count total parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = count_parameters(model)\n",
    "# print human readable number of parameters\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50258, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50258, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mechan* (\n"
     ]
    }
   ],
   "source": [
    "# autoregressive generation example\n",
    "\n",
    "# Prepare the prompt\n",
    "prompt = \"mechan*\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate text\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_new_tokens=1,\n",
    "        temperature=1.0,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "# Decode and print the result\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: The best way to learn programming is to apply\n",
      "Step 2: The best way to learn programming is to apply it\n",
      "Step 3: The best way to learn programming is to apply it to\n",
      "Step 4: The best way to learn programming is to apply it to a\n",
      "Step 5: The best way to learn programming is to apply it to a project\n",
      "Step 6: The best way to learn programming is to apply it to a project.\n",
      "Step 7: The best way to learn programming is to apply it to a project. If\n",
      "Step 8: The best way to learn programming is to apply it to a project. If you\n",
      "Step 9: The best way to learn programming is to apply it to a project. If you have\n",
      "Step 10: The best way to learn programming is to apply it to a project. If you have never\n",
      "Step 11: The best way to learn programming is to apply it to a project. If you have never tried\n",
      "Step 12: The best way to learn programming is to apply it to a project. If you have never tried to\n",
      "Step 13: The best way to learn programming is to apply it to a project. If you have never tried to learn\n",
      "Step 14: The best way to learn programming is to apply it to a project. If you have never tried to learn programming\n",
      "Step 15: The best way to learn programming is to apply it to a project. If you have never tried to learn programming in\n",
      "Step 16: The best way to learn programming is to apply it to a project. If you have never tried to learn programming in front\n",
      "Step 17: The best way to learn programming is to apply it to a project. If you have never tried to learn programming in front of\n",
      "Step 18: The best way to learn programming is to apply it to a project. If you have never tried to learn programming in front of your\n",
      "Step 19: The best way to learn programming is to apply it to a project. If you have never tried to learn programming in front of your eyes\n",
      "Step 20: The best way to learn programming is to apply it to a project. If you have never tried to learn programming in front of your eyes,\n"
     ]
    }
   ],
   "source": [
    "# autoregressive generation, ONE TOKEN AT A TIME\n",
    "\n",
    "# Prepare the prompt\n",
    "prompt = \"The best way to learn programming is to\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Start with the input tokens\n",
    "current_tokens = inputs.input_ids\n",
    "\n",
    "# Generate tokens one by one in a loop\n",
    "max_new_tokens = 20  # How many new tokens to generate\n",
    "with torch.no_grad():\n",
    "    for i in range(max_new_tokens):\n",
    "        # Generate just 1 token\n",
    "        outputs = model.generate(\n",
    "            current_tokens,\n",
    "            max_new_tokens=1,\n",
    "            temperature=1.0,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "        \n",
    "        # Update current_tokens with the new token\n",
    "        current_tokens = outputs\n",
    "        \n",
    "        # Decode and print the current state\n",
    "        generated_text = tokenizer.decode(current_tokens[0], skip_special_tokens=True)\n",
    "        print(f\"Step {i+1}: {generated_text}\")\n",
    "        \n",
    "        # Optional: Stop if we hit the EOS token\n",
    "        if outputs[0, -1].item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "# # Final result\n",
    "# final_text = tokenizer.decode(current_tokens[0], skip_special_tokens=True)\n",
    "# print(f\"\\nFinal: {final_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stochasticity of generation, temperature control\n",
    "\n",
    "# autoregressive generation example\n",
    "\n",
    "# Prepare the prompt\n",
    "prompt = \"The best way to learn programming is to\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate text\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_new_tokens=100,\n",
    "        temperature=1.0,        # TRY CHANGING THIS\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "# Decode and print the result\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is how to build a high-grade explosive:\n",
      "\n",
      "In the above pictures, it can be seen that we can make three different high-grade explosives:\n",
      "\n",
      "The first one is the low grade explosive. The second one is the high grade explosive. The third one is the high grade explosive. All three explosives are produced in a single process and are the same composition. So, we have 3 different high-grade explosive compositions, so we can build them together in a single process.\n",
      "\n",
      "To see how it works, we simply need\n"
     ]
    }
   ],
   "source": [
    "# capabilities: in-context learning, safety, refusal (ChatGPT)\n",
    "\n",
    "# base models are happy to generate unsafe / toxic / harmful content:\n",
    "# Prepare the prompt\n",
    "prompt = \"Here is how to build a high-grade explosive:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate text\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_new_tokens=100,\n",
    "        temperature=1.0,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "# Decode and print the result\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'The name of the capital of France is'\n",
      "\n",
      "Top 10 most likely next tokens:\n",
      "----------------------------------------\n",
      " 1. ' Paris' - 0.0478 (4.78%)\n",
      " 2. ' Marse' - 0.0433 (4.33%)\n",
      " 3. ' Saint' - 0.0308 (3.08%)\n",
      " 4. ' the' - 0.0282 (2.82%)\n",
      " 5. ' \"' - 0.0252 (2.52%)\n",
      " 6. ' France' - 0.0240 (2.40%)\n",
      " 7. ' not' - 0.0214 (2.14%)\n",
      " 8. ' French' - 0.0169 (1.69%)\n",
      " 9. ' Le' - 0.0162 (1.62%)\n",
      "10. ' '' - 0.0107 (1.07%)\n",
      "\n",
      "Probability distribution stats:\n",
      "Max probability: 0.0478\n",
      "Min probability: 0.000000\n",
      "Entropy: 6.62\n",
      "Total vocab size: 50258\n"
     ]
    }
   ],
   "source": [
    "# restrict to a single output token, examine next token probability distribution\n",
    "\n",
    "# Prepare the prompt\n",
    "prompt = \"The name of the capital of France is\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs.input_ids).logits[0, -1, :]\n",
    "    next_token_probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "# Get top k most likely tokens (e.g., top 10)\n",
    "top_k = 10\n",
    "top_probs, top_indices = torch.topk(next_token_probs, top_k)\n",
    "\n",
    "# Decode and display the most likely next tokens\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(f\"\\nTop {top_k} most likely next tokens:\")\n",
    "print(\"-\" * 40)\n",
    "for i, (prob, token_id) in enumerate(zip(top_probs, top_indices)):\n",
    "    token = tokenizer.decode(token_id.item())\n",
    "    print(f\"{i+1:2d}. '{token}' - {prob.item():.4f} ({prob.item()*100:.2f}%)\")\n",
    "\n",
    "# Show full probability distribution statistics\n",
    "print(f\"\\nProbability distribution stats:\")\n",
    "print(f\"Max probability: {next_token_probs.max().item():.4f}\")\n",
    "print(f\"Min probability: {next_token_probs.min().item():.6f}\")\n",
    "print(f\"Entropy: {-(next_token_probs * torch.log(next_token_probs + 1e-10)).sum().item():.2f}\")\n",
    "print(f\"Total vocab size: {len(next_token_probs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASIC STATS\n",
      "Vocabulary size: 50,257\n"
     ]
    }
   ],
   "source": [
    "# examine the tokenizer: vocabulary, special tokens, example of tokenizing a string\n",
    "\n",
    "\n",
    "# 1. Basic vocabulary statistics\n",
    "print(f\"BASIC STATS\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM VOCABULARY SAMPLES\n",
      "Token ID | Token | Decoded\n",
      "-----------------------------------\n",
      "   33077 | 'ethy'\n",
      "    2474 | '!\"'\n",
      "   18887 | ' youngest'\n",
      "   23168 | ' embodiment'\n",
      "   19812 | ' fictional'\n",
      "   30669 | ' masterpiece'\n",
      "   26290 | '!),'\n",
      "    4025 | ' larger'\n",
      "   29556 | ' flock'\n",
      "   20498 | ' EVER'\n",
      "   46912 | 'Matrix'\n",
      "   43503 | ' Lime'\n",
      "   18289 | ' Monitor'\n",
      "   35015 | ' Eisenhower'\n",
      "   39355 | '�'\n"
     ]
    }
   ],
   "source": [
    "print(f\"RANDOM VOCABULARY SAMPLES\")\n",
    "print(\"Token ID | Token | Decoded\")\n",
    "print(\"-\" * 35)\n",
    "for _ in range(15):\n",
    "    random_id = random.randint(0, tokenizer.vocab_size - 1)\n",
    "    token = tokenizer.convert_ids_to_tokens(random_id)\n",
    "    decoded = tokenizer.decode(random_id)\n",
    "    # print(f\"{random_id:8d} | {token:8s} | '{decoded}'\")\n",
    "    print(f\"{random_id:8d} | '{decoded}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoded tokens for '2035345345345345345345':\n",
      "[1238, 2327, 27712, 27712, 27712, 27712, 27712, 27712]\n",
      "Decoded tokens:\n",
      "Token ID 1238: '20'\n",
      "Token ID 2327: '35'\n",
      "Token ID 27712: '345'\n",
      "Token ID 27712: '345'\n",
      "Token ID 27712: '345'\n",
      "Token ID 27712: '345'\n",
      "Token ID 27712: '345'\n",
      "Token ID 27712: '345'\n"
     ]
    }
   ],
   "source": [
    "# encode a string into tokens\n",
    "# strng = \"Hello, world! This is a test string.\"\n",
    "strng = \"2035345345345345345345\"\n",
    "encoded = tokenizer.encode(strng)\n",
    "print(f\"\\nEncoded tokens for '{strng}':\")\n",
    "print(encoded)\n",
    "# print what each token ID corresponds to\n",
    "decoded_tokens = [tokenizer.decode([token_id]) for token_id in encoded]\n",
    "print(\"Decoded tokens:\")\n",
    "for token_id, decoded in zip(encoded, decoded_tokens):\n",
    "    print(f\"Token ID {token_id}: '{decoded}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50258, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50258, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the model architecture: overall embedding/unembedding, blocks, attention, MLP \n",
    "# Note: A detailed model implementation can be found in Karpathy's MinGPT implementation: \n",
    "# https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n",
    "\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50258, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.wte.weight.shape  # Word token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transformer.h[0].attn.c_attn # Q, K, V in self-attention, all together in ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
