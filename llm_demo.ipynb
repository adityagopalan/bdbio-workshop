{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and moved to device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Load GPT-2 model and tokenizer directly\n",
    "model_id = \"gpt2\"  # This is the 124M parameter version\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float32,  # GPT-2 small works fine in full precision\n",
    ")\n",
    "\n",
    "# Add a proper pad token for GPT-2\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(\"Model loaded and moved to device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best way to learn programming is to apply it at your own pace. Start at 4.5 minutes, then gradually increase it, and\n"
     ]
    }
   ],
   "source": [
    "# Prepare the prompt\n",
    "prompt = \"The best way to learn programming is to\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate text\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_new_tokens=20,\n",
    "        temperature=1.0,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "# Decode and print the result\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: The best way to learn programming is to study\n",
      "Step 2: The best way to learn programming is to study the\n",
      "Step 3: The best way to learn programming is to study the basic\n",
      "Step 4: The best way to learn programming is to study the basic concepts\n",
      "Step 5: The best way to learn programming is to study the basic concepts,\n",
      "Step 6: The best way to learn programming is to study the basic concepts, not\n",
      "Step 7: The best way to learn programming is to study the basic concepts, not just\n",
      "Step 8: The best way to learn programming is to study the basic concepts, not just the\n",
      "Step 9: The best way to learn programming is to study the basic concepts, not just the basics\n",
      "Step 10: The best way to learn programming is to study the basic concepts, not just the basics,\n"
     ]
    }
   ],
   "source": [
    "# autoregressive generation, one token at a time\n",
    "\n",
    "# Prepare the prompt\n",
    "prompt = \"The best way to learn programming is to\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Start with the input tokens\n",
    "current_tokens = inputs.input_ids\n",
    "\n",
    "# Generate tokens one by one in a loop\n",
    "max_new_tokens = 10  # How many new tokens to generate\n",
    "with torch.no_grad():\n",
    "    for i in range(max_new_tokens):\n",
    "        # Generate just 1 token\n",
    "        outputs = model.generate(\n",
    "            current_tokens,\n",
    "            max_new_tokens=1,\n",
    "            temperature=1.0,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "        \n",
    "        # Update current_tokens with the new token\n",
    "        current_tokens = outputs\n",
    "        \n",
    "        # Decode and print the current state\n",
    "        generated_text = tokenizer.decode(current_tokens[0], skip_special_tokens=True)\n",
    "        print(f\"Step {i+1}: {generated_text}\")\n",
    "        \n",
    "        # Optional: Stop if we hit the EOS token\n",
    "        if outputs[0, -1].item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "# # Final result\n",
    "# final_text = tokenizer.decode(current_tokens[0], skip_special_tokens=True)\n",
    "# print(f\"\\nFinal: {final_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stochasticity of generation, temperature control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is how to build a high-grade explosive:\n",
      "\n",
      "The basic process of building a high-grade explosive is to first go through a number of steps (1-4) to build a very dense, highly explosive explosive. The next step is to apply pressure to the surface. For example, if you are building a bomb in the air, apply a pressure of 5 mm at the detonator head to a layer of the earth that is already a solid block of earth (e.g., a small lake). Apply a second pressure to the\n"
     ]
    }
   ],
   "source": [
    "# capabilities: in-context learning, safety, refusal\n",
    "\n",
    "# Prepare the prompt\n",
    "prompt = \"Here is how to build a high-grade explosive:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate text\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_new_tokens=100,\n",
    "        temperature=1.0,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "# Decode and print the result\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'The capital of France is'\n",
      "\n",
      "Top 10 most likely next tokens:\n",
      "----------------------------------------\n",
      " 1. ' the' - 0.0846 (8.46%)\n",
      " 2. ' now' - 0.0479 (4.79%)\n",
      " 3. ' a' - 0.0462 (4.62%)\n",
      " 4. ' France' - 0.0324 (3.24%)\n",
      " 5. ' Paris' - 0.0322 (3.22%)\n",
      " 6. ' in' - 0.0266 (2.66%)\n",
      " 7. ' also' - 0.0264 (2.64%)\n",
      " 8. ' not' - 0.0238 (2.38%)\n",
      " 9. ' home' - 0.0233 (2.33%)\n",
      "10. ' still' - 0.0155 (1.55%)\n",
      "\n",
      "Probability distribution stats:\n",
      "Max probability: 0.0846\n",
      "Min probability: 0.000000\n",
      "Entropy: 6.00\n",
      "Total vocab size: 50258\n"
     ]
    }
   ],
   "source": [
    "# restrict to a single output token, examine next token probability distribution\n",
    "\n",
    "# Prepare the prompt\n",
    "prompt = \"The capital of France is\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs.input_ids).logits[0, -1, :]\n",
    "    next_token_probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "# Get top k most likely tokens (e.g., top 10)\n",
    "top_k = 10\n",
    "top_probs, top_indices = torch.topk(next_token_probs, top_k)\n",
    "\n",
    "# Decode and display the most likely next tokens\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(f\"\\nTop {top_k} most likely next tokens:\")\n",
    "print(\"-\" * 40)\n",
    "for i, (prob, token_id) in enumerate(zip(top_probs, top_indices)):\n",
    "    token = tokenizer.decode(token_id.item())\n",
    "    print(f\"{i+1:2d}. '{token}' - {prob.item():.4f} ({prob.item()*100:.2f}%)\")\n",
    "\n",
    "# Show full probability distribution statistics\n",
    "print(f\"\\nProbability distribution stats:\")\n",
    "print(f\"Max probability: {next_token_probs.max().item():.4f}\")\n",
    "print(f\"Min probability: {next_token_probs.min().item():.6f}\")\n",
    "print(f\"Entropy: {-(next_token_probs * torch.log(next_token_probs + 1e-10)).sum().item():.2f}\")\n",
    "print(f\"Total vocab size: {len(next_token_probs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASIC STATS\n",
      "Vocabulary size: 50,257\n"
     ]
    }
   ],
   "source": [
    "# examine the tokenizer: vocabulary, special tokens, example of tokenizing a string\n",
    "\n",
    "\n",
    "# 1. Basic vocabulary statistics\n",
    "print(f\"BASIC STATS\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM VOCABULARY SAMPLES\n",
      "Token ID | Token | Decoded\n",
      "-----------------------------------\n",
      "   42953 | ' TRUMP'\n",
      "   37815 | 'CONT'\n",
      "   30249 | ' Cunning'\n",
      "   28999 | ' classmates'\n",
      "    4363 | 'resp'\n",
      "    2728 | ' cause'\n",
      "   16144 | ' alias'\n",
      "   46795 | ' gib'\n",
      "   33487 | 'erville'\n",
      "    6658 | ' vac'\n",
      "    7906 | ' spin'\n",
      "    8026 | ' Stone'\n",
      "   34881 | ' CNBC'\n",
      "   23114 | 'Display'\n",
      "   30577 | ' strut'\n"
     ]
    }
   ],
   "source": [
    "print(f\"RANDOM VOCABULARY SAMPLES\")\n",
    "print(\"Token ID | Token | Decoded\")\n",
    "print(\"-\" * 35)\n",
    "for _ in range(15):\n",
    "    random_id = random.randint(0, tokenizer.vocab_size - 1)\n",
    "    token = tokenizer.convert_ids_to_tokens(random_id)\n",
    "    decoded = tokenizer.decode(random_id)\n",
    "    # print(f\"{random_id:8d} | {token:8s} | '{decoded}'\")\n",
    "    print(f\"{random_id:8d} | '{decoded}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoded tokens for 'Hello, world! This is a test string.':\n",
      "[15496, 11, 995, 0, 770, 318, 257, 1332, 4731, 13]\n",
      "Decoded tokens:\n",
      "Token ID 15496: 'Hello'\n",
      "Token ID 11: ','\n",
      "Token ID 995: ' world'\n",
      "Token ID 0: '!'\n",
      "Token ID 770: ' This'\n",
      "Token ID 318: ' is'\n",
      "Token ID 257: ' a'\n",
      "Token ID 1332: ' test'\n",
      "Token ID 4731: ' string'\n",
      "Token ID 13: '.'\n"
     ]
    }
   ],
   "source": [
    "# encode a string into tokens\n",
    "strng = \"Hello, world! This is a test string.\"\n",
    "encoded = tokenizer.encode(strng)\n",
    "print(f\"\\nEncoded tokens for '{strng}':\")\n",
    "print(encoded)\n",
    "# print what each token ID corresponds to\n",
    "decoded_tokens = [tokenizer.decode([token_id]) for token_id in encoded]\n",
    "print(\"Decoded tokens:\")\n",
    "for token_id, decoded in zip(encoded, decoded_tokens):\n",
    "    print(f\"Token ID {token_id}: '{decoded}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50258, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50258, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the model architecture: overall embedding/unembedding, blocks, attention, MLP \n",
    "\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1101, -0.0393,  0.0331,  ..., -0.1364,  0.0151,  0.0453],\n",
       "        [ 0.0403, -0.0486,  0.0462,  ...,  0.0861,  0.0025,  0.0432],\n",
       "        [-0.1275,  0.0479,  0.1841,  ...,  0.0899, -0.1297, -0.0879],\n",
       "        ...,\n",
       "        [ 0.1860,  0.0167,  0.0461,  ..., -0.0963,  0.0785, -0.0225],\n",
       "        [ 0.0514, -0.0277,  0.0499,  ...,  0.0070,  0.1552,  0.1207],\n",
       "        [-0.0026, -0.0585,  0.1174,  ...,  0.0236,  0.0039,  0.0344]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.wte.weight  # Word token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv1D(nf=2304, nx=768)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.h[0].attn.c_attn # Q, K, V in self-attention, all together in one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
